# Plan Reviewer Agent Design (v1.0)

## 1. Overview
The **Plan Reviewer Agent** is a new component in the AI Scientist system designed to act as a quality assurance layer between the **Planner Agent** and the **Executor Agent**. Its primary goal is to validate, critique, and potentially reject experiment plans generated by the Planner before they are executed. This ensures that resources are not wasted on invalid, unsafe, or strategically poor experiments.

## 2. System Architecture

The Reviewer Agent will be a standalone agent, similar to the Planner and Analyzer.

### Class Definition Strategy
```python
class PlanReviewerAgent:
    def __init__(self, llm_client: LLMClient, design_space: Dict[str, Any]):
        ...

    def review_plan(
        self,
        proposed_configs: List[SCIConfiguration],
        context: Dict[str, Any]
    ) -> ReviewResult:
        ...
```

### Data Structures
We need a structure to hold the review result:
```python
@dataclass
class ReviewResult:
    approved: bool
    approved_configs: List[SCIConfiguration]  # Subset of original configs that passed
    feedback: str  # Detailed feedback for the Planner if rejection occurs
    critique: Dict[str, str]  # Per-config critique
```

## 3. Review Logic Criteria

The reviewer will employ a two-stage review process:

### Stage 1: Fast Rule-based Verification (Sanity Checks)
Before calling the LLM, perform cheap checks:
- **Design Space Compliance**: Are all parameters strictly within the `design_space`? (Double-check in case LLM hallucinated values).
- **Resource Constraints**: Does the estimated computational cost exceed limits? (e.g., extremely high resolution or epochs).
- **Syntax/Format**: Are all required fields present?

### Stage 2: LLM-based Strategic Review
Uses the LLM to evaluate the *quality* and *intent* of the plan.
- **Safety**: Is the experiment dangerous or likely to crash? (e.g., Learning rate too high for a deep network).
- **Novelty**: Does the plan actually explore new areas or just minor variations of known failures?
- **Alignment**: Does the plan align with the current goal (Exploration vs. Exploitation)?
    - *Example*: If we are in early stages, is the planner exploring enough? If late stage, is it converging?
- **Scientific Logic**: Is the hypothesis behind the config sound?

## 4. Interaction Workflow

Current Flow:
`Planner` -> `Configs` -> `Executor`

**Proposed Flow with Reviewer:**

1. **Planner** generates `Draft Configs`.
2. **Reviewer** receives `Draft Configs` + `Context` (Design Space, History Summary).
3. **Reviewer** performs **Stage 1 Checks**.
    - If fail: Return hard rejection immediately.
4. **Reviewer** performs **Stage 2 Checks** (LLM).
    - The LLM acts as a "Senior Principal Investigator" reviewing a student's proposal.
5. **Decision**:
    - **Approve**: Pass configs to Executor.
    - **Partial Approve**: Filter out bad configs, pass good ones to Executor.
    - **Reject**: Return `feedback` string to Planner.
6. **(Optional Loop)**: Planner receives feedback and regenerates plan (Max Retries = 3).

## 5. Reviewer Prompt Design

The prompt should frame the LLM as a critical reviewer.

**System Prompt:**
> "You are a Senior Reviewer for scientific experiments in Snapshot Compressive Imaging. Your job is to strictly evaluate proposed experiment configurations for safety, validity, and scientific value. You are skeptical and prioritize resource efficiency."

**User Prompt Template:**
> **Proposed Plan:**
> [List of Configs]
>
> **Context:**
> - Current Cycle: {cycle}
> - Best found so far: {best_metric}
> - Recent failures: {recent_failures}
>
> **Review Task:**
> 1. Identify any unsafe parameters (e.g., LR > 1e-2).
> 2. Check if these experiments needlessly repeat historical failures.
> 3. Verify if the exploration strategy makes sense for the current cycle.
>
> **Verdict:**
> For each config, assign: [APPROVED | REJECTED]
> If REJECTED, provide a specific reason.

## 6. Integration into AIScientist

Modify `src/sci_scientist/core/scientist.py`:

```python
# In __init__
self.reviewer = PlanReviewerAgent(llm_client, design_space)

# In run_async loop
# ... after planner.plan_experiments ...

new_configs = self.planner.plan_experiments(...)

# === NEW REVIEW STEP ===
review_result = await self.reviewer.review_plan(new_configs, summary)

if review_result.approved:
    configs_to_run = review_result.approved_configs
    logger.info(f"Plan approved: {len(configs_to_run)} configs")
else:
    logger.warning(f"Plan rejected/modified. Feedback: {review_result.feedback}")
    # Option A: Skip this cycle
    # Option B: Run only approved ones
    configs_to_run = review_result.approved_configs

    # Optional: Re-planning could happen here
    if not configs_to_run and review_result.feedback:
         # Retry planning with feedback...
         pass

# ... proceed to execute configs_to_run ...
```

## 7. Future Extensions
- **Human-in-the-loop**: For high-cost experiments, the Reviewer could pause and wait for human approval via CLI/UI.
- **Cost Estimation Model**: Integrate a predictive model to estimate GPU hours and reject expensive runs automatically.
